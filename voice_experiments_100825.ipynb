{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPL6jAKelwLu6gPN+KBUH+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikoschenk/higgs-audio/blob/main/voice_experiments_100825.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VkTESqp1MkS"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clone and change into repo directory & install requirements.\n",
        "!git clone https://github.com/Nikoschenk/higgs-audio.git\n",
        "%cd higgs-audio\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id=\"bosonai/higgs-audio-v2-generation-3B-base\", local_dir=\"model\")"
      ],
      "metadata": {
        "id": "KA964u9n3TdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "qbLMqL2M1lOS",
        "outputId": "85ffc09e-da2f-4b1a-c6e0-297798749f07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e612e225-462a-421a-8a2c-202ee82c09fc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e612e225-462a-421a-8a2c-202ee82c09fc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ni_de.txt to ni_de.txt\n",
            "Saving ni_de.wav to ni_de.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "print(os.listdir())\n",
        "shutil.move(\"ni_de.txt\", \"higgs-audio/examples/voice_prompts/ni_de.txt\")\n",
        "shutil.move(\"ni_de.wav\", \"higgs-audio/examples/voice_prompts/ni_de.wav\")\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "051-XvIu2Q5m",
        "outputId": "3cf270cd-2b23-47fb-8c3c-c004c3a06caf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'model', 'ni_de.wav', 'higgs-audio', 'ni_de.txt', 'sample_data']\n",
            "['.config', 'model', 'higgs-audio', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd higgs-audio\n",
        "print(os.listdir())\n",
        "!python examples/generation.py \\\n",
        "  --model_path bosonai/higgs-audio-v2-generation-3B-base \\\n",
        "  --audio_tokenizer bosonai/higgs-audio-v2-tokenizer \\\n",
        "  --transcript examples/voice_prompts/ni_de.txt \\\n",
        "  --ref_audio examples/voice_prompts/ni_de.wav \\\n",
        "  --seed 7788 \\\n",
        "  --out_path output.wav \\\n",
        "  --device auto \\\n",
        "  --use_static_kv_cache 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcTO99f82YKL",
        "outputId": "52be192f-e7dd-4350-a9c1-ce9d171954d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/higgs-audio\n",
            "['setup.py', '.gitignore', '.git', 'LICENSE', 'examples', 'figures', 'pyproject.toml', 'higgs-audio', 'setup.cfg', '.gitmodules', 'tech_blogs', 'requirements.txt', 'boson_multimodal', 'README.md', '.github']\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "Fetching 6 files:   0% 0/6 [00:00<?, ?it/s]\n",
            ".gitattributes: 1.64kB [00:00, 9.30MB/s]\n",
            "Fetching 6 files:  17% 1/6 [00:00<00:00,  9.77it/s]\n",
            "LICENSE: 9.17kB [00:00, 36.8MB/s]\n",
            "\n",
            "README.md: 14.5kB [00:00, 29.9MB/s]\n",
            "\n",
            "config.json: 100% 226/226 [00:00<00:00, 1.87MB/s]\n",
            "\n",
            "higgs_audio_tokenizer_architecture.png:   0% 0.00/117k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model.pth:   0% 0.00/806M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "higgs_audio_tokenizer_architecture.png: 100% 117k/117k [00:00<00:00, 351kB/s]\n",
            "Fetching 6 files:  83% 5/6 [00:00<00:00, 10.50it/s]\n",
            "\n",
            "model.pth:   0% 1.25M/806M [00:00<07:34, 1.77MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   8% 68.3M/806M [00:01<00:14, 49.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  25% 202M/806M [00:01<00:03, 156MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "model.pth:  42% 337M/806M [00:01<00:01, 279MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  67% 538M/806M [00:02<00:00, 505MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth: 100% 806M/806M [00:02<00:00, 374MB/s]\n",
            "Fetching 6 files: 100% 6/6 [00:02<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "config.json: 1.38kB [00:00, 8.24MB/s]\n",
            "model.safetensors: 100% 378M/378M [00:01<00:00, 282MB/s]\n",
            "\u001b[32m2025-08-10 12:11:13.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mUsing device: cpu\u001b[0m\n",
            "config.json: 6.26kB [00:00, 25.4MB/s]\n",
            "model.safetensors.index.json: 31.1kB [00:00, 111MB/s]\n",
            "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 645k/4.97G [00:01<2:24:23, 573kB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 69.2M/4.97G [00:01<01:30, 54.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 337M/4.97G [00:02<00:18, 245MB/s]  \u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 539M/4.97G [00:07<01:02, 70.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 740M/4.97G [00:07<00:36, 116MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 941M/4.97G [00:07<00:23, 175MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.97G [00:07<00:15, 252MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.97G [00:07<00:11, 312MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.97G [00:07<00:09, 376MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.61G/4.97G [00:07<00:06, 524MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.75G/4.97G [00:07<00:05, 604MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.88G/4.97G [00:08<00:05, 574MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.22G/4.97G [00:08<00:02, 963MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.42G/4.97G [00:08<00:02, 1.08GB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.62G/4.97G [00:08<00:02, 1.10GB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.82G/4.97G [00:08<00:01, 1.08GB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.02G/4.97G [00:09<00:01, 1.02GB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.16G/4.97G [00:09<00:01, 930MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.29G/4.97G [00:09<00:01, 947MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.43G/4.97G [00:09<00:01, 986MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.63G/4.97G [00:10<00:02, 616MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.76G/4.97G [00:11<00:04, 274MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.83G/4.97G [00:11<00:04, 263MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.90G/4.97G [00:11<00:04, 264MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 3.96G/4.97G [00:12<00:03, 286MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.03G/4.97G [00:12<00:02, 318MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.10G/4.97G [00:12<00:02, 358MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.16G/4.97G [00:12<00:01, 406MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.23G/4.97G [00:12<00:01, 451MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.37G/4.97G [00:12<00:01, 548MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.50G/4.97G [00:12<00:00, 616MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.63G/4.97G [00:13<00:00, 665MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.76G/4.97G [00:13<00:00, 700MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.97G/4.97G [00:13<00:00, 369MB/s]\n",
            "Downloading shards:  33% 1/3 [00:13<00:27, 13.55s/it]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 18.5M/4.98G [00:01<04:35, 18.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 245M/4.98G [00:01<00:22, 207MB/s]  \u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 379M/4.98G [00:01<00:14, 326MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 495M/4.98G [00:01<00:14, 308MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 562M/4.98G [00:02<00:13, 319MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 629M/4.98G [00:02<00:16, 265MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 983M/4.98G [00:03<00:09, 442MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.05G/4.98G [00:08<00:56, 69.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.25G/4.98G [00:08<00:34, 109MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.32G/4.98G [00:11<00:51, 71.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.52G/4.98G [00:11<00:31, 109MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.59G/4.98G [00:12<00:30, 111MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.65G/4.98G [00:13<00:32, 101MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.72G/4.98G [00:14<00:34, 93.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.79G/4.98G [00:14<00:33, 96.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.83G/4.98G [00:15<00:32, 98.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.90G/4.98G [00:16<00:30, 102MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.93G/4.98G [00:16<00:29, 102MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.99G/4.98G [00:16<00:29, 102MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.06G/4.98G [00:17<00:29, 100MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.13G/4.98G [00:18<00:25, 110MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.18G/4.98G [00:18<00:31, 90.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.24G/4.98G [00:22<01:02, 43.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.38G/4.98G [00:22<00:32, 81.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.44G/4.98G [00:23<00:38, 65.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.51G/4.98G [00:27<01:00, 40.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.58G/4.98G [00:29<01:05, 36.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.58G/4.98G [00:40<01:05, 36.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.65G/4.98G [00:47<03:43, 10.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.85G/4.98G [00:47<01:32, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.05G/4.98G [00:50<00:56, 34.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.11G/4.98G [00:51<00:56, 33.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.17G/4.98G [00:52<00:45, 39.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.24G/4.98G [00:53<00:38, 45.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.31G/4.98G [00:53<00:31, 53.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.37G/4.98G [00:54<00:26, 60.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.44G/4.98G [00:54<00:22, 68.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.51G/4.98G [00:55<00:19, 75.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.57G/4.98G [00:56<00:16, 87.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.64G/4.98G [00:56<00:14, 91.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.71G/4.98G [00:57<00:12, 104MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.78G/4.98G [00:57<00:11, 104MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.84G/4.98G [00:58<00:09, 114MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.91G/4.98G [00:58<00:09, 111MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.98G/4.98G [00:59<00:09, 109MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.04G/4.98G [00:59<00:07, 119MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.11G/4.98G [01:00<00:07, 114MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.18G/4.98G [01:01<00:09, 83.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.28G/4.98G [01:02<00:05, 123MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.35G/4.98G [01:02<00:05, 117MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.42G/4.98G [01:03<00:05, 113MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.45G/4.98G [01:03<00:04, 116MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.51G/4.98G [01:04<00:04, 111MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.58G/4.98G [01:04<00:03, 122MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.65G/4.98G [01:05<00:02, 115MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.71G/4.98G [01:06<00:02, 111MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.78G/4.98G [01:06<00:01, 121MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.85G/4.98G [01:07<00:01, 115MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.92G/4.98G [01:07<00:00, 111MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.98G/4.98G [01:08<00:00, 72.8MB/s]\n",
            "Downloading shards:  67% 2/3 [01:22<00:45, 45.90s/it]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/1.59G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 678k/1.59G [00:01<41:40, 637kB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 202M/1.59G [00:01<00:09, 141MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 269M/1.59G [00:08<00:50, 26.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 403M/1.59G [00:09<00:27, 43.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 470M/1.59G [00:10<00:21, 53.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 537M/1.59G [00:10<00:17, 61.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 604M/1.59G [00:11<00:13, 72.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 671M/1.59G [00:11<00:11, 79.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 723M/1.59G [00:13<00:16, 52.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 924M/1.59G [00:13<00:05, 117MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 991M/1.59G [00:14<00:05, 114MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 1.06G/1.59G [00:15<00:04, 111MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 1.12G/1.59G [00:15<00:03, 119MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 1.19G/1.59G [00:16<00:03, 119MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 1.26G/1.59G [00:16<00:02, 114MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 1.33G/1.59G [00:17<00:02, 130MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 1.39G/1.59G [00:17<00:01, 166MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 1.46G/1.59G [00:17<00:00, 207MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 1.53G/1.59G [00:17<00:00, 253MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 1.59G/1.59G [00:17<00:00, 90.0MB/s]\n",
            "Downloading shards: 100% 3/3 [01:39<00:00, 33.30s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:00<00:00,  3.89it/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 1.36MB/s]\n",
            "tokenizer_config.json: 50.3kB [00:00, 141MB/s]\n",
            "tokenizer.json: 100% 17.2M/17.2M [00:00<00:00, 34.8MB/s]\n",
            "special_tokens_map.json: 100% 301/301 [00:00<00:00, 2.03MB/s]\n",
            "\u001b[32m2025-08-10 12:12:56.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m690\u001b[0m - \u001b[1mLoading transcript from examples/voice_prompts/ni_de.txt\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/higgs-audio/examples/generation.py\", line 768, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1442, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1363, in main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1226, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 794, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/higgs-audio/examples/generation.py\", line 730, in main\n",
            "    messages, audio_ids = prepare_generation_context(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/higgs-audio/examples/generation.py\", line 437, in prepare_generation_context\n",
            "    assert os.path.exists(prompt_audio_path), (\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: Voice prompt audio file /content/higgs-audio/examples/voice_prompts/examples/voice_prompts/ni_de.wav.wav does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listen to the audio.\n",
        "from IPython.display import Audio\n",
        "Audio(\"output.wav\", rate=24000)\n",
        "# Save the audio.\n",
        "from google.colab import files\n",
        "files.download(\"output.wav\")"
      ],
      "metadata": {
        "id": "xt_Zlu7u5Ntp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}